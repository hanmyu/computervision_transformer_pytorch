개념 요약을 쪽지시험 방식으로 진행했습니다.
## <쪽지시험 문제>

1. 편향-분산 트레이드오프란?
2. 배치정규화를 하는 이유는?
3. 정규화의 종류 4가지를 말하시오
4. 가중치 초기화가 필요한 이유는?
5. 가중치 초기화가 종류 5가지를 말하시오
6. 정칙화(Regularization)를 하는 이유는?
7. L1 정칙화와 L2 정칙화의 차이를 말하시오.
8. 모멘텀을 사용하는 이유는/모멘텀이 traditional 경사 하강법 알고리즘보다 나은 이유는?
9. 엘라스틱넷을 사용하는 경우는 언제인가? 엘라스틱 넷을 사용하면 나타나는 단점은?
10. 드롭아웃이란?
11. 데이터 증강이란?
12. 백본이란?
13. 전이학습이란?

## <쪽지시험 답변>

1. **편향-분산 트레이드오프란?**   
**이상적인 모델: 낮은 편향, 낮은 분산 필요**    
이해를 돕는 영상 추천!!: https://www.youtube.com/watch?v=GR6U0a9IjHE 
- **[Case 1] 높은 편향, 낮은 분산**(high bias, low variance): 데이터가 항상 일정한 값을 가질 확률 높음(bias), 데이터 특징을 제대로 학습하지 못해 과소적합     
- **[Case 2] 낮은 편향, 높은 분산**(low bias, high variance): 데이터의 변동 폭이 큼, 데이터의 노이즈까지 학습 과정에 포함돼 과대적합     
**→ 손실함수에서 가장 에러가 낮은 지점을 찾아야함 (편향, 분산의 optimal point 찾기)**
<img width="400" alt="스크린샷 2024-01-16 오후 3 49 29" src="https://github.com/hanmyu/computervision_transformer_pytorch/assets/157959298/ad6ed9c4-0f12-4a4c-a68a-f19eaf063e96">
     
(자료: https://medium.com/@ivanreznikov/stop-using-the-same-image-in-bias-variance-trade-off-explanation-691997a94a54)     

2. **배치정규화를 하는 이유는?**    
배치에 전달되는 입력 값이 [100, 1, 1] 이거나 [1, 0.01, 0.01]일 때, 배치 정규화를 시키면 둘 다 [1.4142, -0.7071, -0.7071]이 된다.    
만약 [100, 1, 1]과 [1, 0.01, 0.01]을 그대로 모델 학습시키는데 사용하면 입력값이 균일하지 않아(너무 차이가 큼) 학습이 불안정해지고 가중치가 일정한 값으로 수렴하기 어려워짐 -> 이런 이유로 배치 정규화가 필요함.

3. **정규화의 종류 4가지를 말하시오**    
**배치 정규화, 계층 정규화, 인스턴트 정규화, 그룹 정규화**      
***이미지의 경우—> Dimension(차원): 이미지 데이터의 크기(너비, 높이), Channels(채널): RGB 색상**      
- 🔸**배치 정규화**: 적은 데이터로 좋은 모델 만들 수 있음(14배 적은 데이터로도 가능)
    - 순방향 신경망(Feedforward Neural Network)에서 많이 쓰임→ 예시: 합성곱 신경망(CNN), 다층 퍼셉트론(MLP)
- 🔸**계층 정규화**: **샘플이 다른 길이를 가지더라도 할 수 있는 정규화 방법**
    - 자연어처리에서 많이 쓰임→ 예시: 순환 신경망(RNN), 트랜스포머(Transformer)
- 🔸**인스턴트 정규화**: **입력이 다른 분포를 가질 때 쓸 수 있는 정규화 방법**
    - 이미지 생성에서 많이 쓰임 → 생성적 적대 신경망(GAN), 스타일 변환(Style Transfer)
- 🔸**그룹 정규화**: 배치 크기가 작거나 채널 수가 매우 많을 때 사용되는 정규화 방법
    - 합성곱 신경망(CNN)의 배치 크기가 작을 때     

*계층 정규화(LN), 인스턴트 정규화(IN) 차이 차이?    
LN normalizes across the features for a single data point in a mini-batch.     
IN normalizes across the instances (samples) for each feature independently.     
While LN normalizes across features for each data point in a batch, IN normalizes across instances for each feature.      
-"feature" typically refers to the individual dimensions or channels of the input data.     
<img width="600" alt="스크린샷 2024-01-16 오후 3 49 29" src="https://github.com/hanmyu/computervision_transformer_pytorch/assets/157959298/d33d80a6-8f2c-4b87-909b-22f5c017134b">

4. **가중치 초기화가 필요한 이유는?**    
**가중치 초기화는 모델의 초기 가중치를 설정하는 걸 말함.**    
**가중치 초기화 안시키면 기울기 폭주나 기울기 소실 문제 심각해질 수 있음.**     
가중치 초기화 장점: 학습 속도 향상(모델 수렴속도 향상)     
*If the initial weights are too large, it can lead to exploding gradients, causing the model to diverge. On the other hand, if the weights are too small, it can result in vanishing gradients, making the learning process extremely slow or preventing it altogether.
     
5. **가중치 초기화의 종류 5가지를 말하시오**
**상수 초기화(Bad), 무작위 초기화(Generally bad, but good if it is a single layer), 제이비어 & 글로럿 초기화(시그모이드, 하이퍼볼릭 탄젠트 사용시), 카이밍 & 허 초기화(ReLU 사용 시 좋음), 직교 초기화(RNN에서 기울기 소실 문제 방지하는데 사용)**
- 🔸**상수 초기화**: 신경망의 모든 노드들을 다 똑같은 값으로 초기화, **역전파 과정에서 모든 뉴런에 동일한 그라디언트 적용, 학습 중 가중치 동일해짐**
    - 문제점: 모든 노드가 동일한 출력 생성(Symmetry Problem) → 모델 학습되지 않음
- 🔸**무작위 초기화**: 신경망의 모든 노드들에게 랜덤한 값으로 초기화, 많이 사용되는 초기화 방법, 계층이 적거나 하나일 때 많이 쓰임.
    - 문제점: **계층이 많아지고 깊어질수록 활성화 값이 양 끝단에 치우치게 되어 기울기 소실 현상 발생** → 모델 제대로 학습되지 않음
- 🔸**제이비어 & 글로럿 초기화**: 신경망의 노드의 출력 분산이 입력 분산과 동일하도록 가중치를 초기화하는 방법(균등 분포, 정규 분포를 사용해 가중치를 초기화), **입력 뉴런+ 출력 뉴런 기반으로 가중치 초기화**, 활성화함수로 **시그모이드나 하이퍼볼릭 탄젠트를 사용할 때 좋음**
- 🔸**카이밍 초기화/허 초기화**: 신경망의 노드의 출력 분산이 입력 분산과 동일하도록 가중치를 초기화하는 방법(균등 분포, 정규 분포를 사용해 가중치를 초기화), 제이비어 & 글로럿 초기화와 비슷하나 **입력 뉴런 수를 기반으로만 가중치 초기화**한다. 활성화 함수로 **ReLU를 사용할 때 좋음.**
    - ***ReLU의 죽은 뉴런 문제를 최소화: 가중치가 너무 작아서 뉴런들이 죽지 않도록 함.***
    - ***Kaiming Initialization aims to keep the variance of the activations roughly constant across different layers of the network.***
    - ***Setting the standard deviation proportional to the square root of the inverse of the number of input units helps mitigate the vanishing gradient problem. This initialization ensures that the weights are not too small, preventing the neurons from becoming inactive.***
    - ***Kaiming Initialization aims to preserve the strength of the signal as it passes through the network, preventing the signal from diminishing too quickly or too slowly***
- 🟡**직교 초기화**: ***설명??*** 기울기 폭주나 기울기 소실 문제 없음, **RNN에서 기울기 소실 문제 방지하는데 사용**     

***ReLU 죽은 뉴런 문제란?**      
**ReLU는 입력이 음수일 경우, 0을 출력함. back propagation하며 모델 훈련하는 도중에 출력이 항상 0인 것이 죽은 뉴런 문제임.**      
ReLU is a popular activation function that introduces non-linearity by outputting the input for positive values and zero for negative values.     
Once a ReLU neuron becomes inactive, it essentially stops learning because the gradient during backpropagation is zero for inputs less than or equal to zero. As a result, the weights associated with the input connections to that neuron no longer receive updates, and the neuron remains inactive throughout training.     
the dead node problem in ReLU occurs when certain neurons become inactive and always output zero during training, leading to the cessation of learning for those neurons      

6. **정칙화(Regularization)를 하는 이유는?**     
과대적합을 방지하기 위해, 암기 방지, 일반화할 수 있도록      
- 암기(Memorization): 훈련 데이터의 노이즈 학습, 모델 학습에 실패하는 것
- 일반화(Generalization): 훈련 데이터의 노이즈 학습 X, 모델 학습 성공 (새로운 데이터에서도 정확한 예측함)      
→ **정칙화(Regularization) 사용 시, 노이즈를 줄여서 분산 값이 낮아진다. 모델이 학습할 때 의존하는 특징의 수를 줄여서 모델 학습 능력 개선**      
**→ 데이터 수가 많거나 데이터가 클린(노이즈 X)할 때는 정칙화 사용 X**      
e.g. L1 정칙화, L2 정칙화, 가중치 감쇠, 드롭아웃 등

7. **L1 정칙화와 L2 정칙화의 차이를 말하시오.**      
공통점: 과대적합 방지      
**지금까지 우리는 손실함수만 보고 모델 학습을 시켰지만 손실함수+ L1 정칙화 or L2 정칙화 값이 최소값이 되는 지점을 찾아서 모델 학습을 시켜야한다.**

|  | L1 정칙화 | L2 정칙화 |
| --- | --- | --- |
| 계산 방식 | 손실함수에 가중치 절댓값의 합 추가 | 손실함수에 가중치 제곱의 합 추가 |
| 모델링 | 희소함, 잘 안쓰임 | 희소하지 않음, 더 많이 쓰임. |
| 특징 선택 | 있음 | 없음 |
| 이상치 | 강함 → 필요없는 것은 0으로 바꿔버릴 수 있기 때문 | 약함 |
| 가중치 | 0이 될 수 있음(몇개의 노드 버림) | 0에 가깝게 됨(노드 다 씀) |
| 학습 | 비교적 복잡한 데이터 패턴 학습 불가 → 몇개의 노드를 버리기 때문에 too simplified 될 수 있음 | 비교적 복잡한 데이터 패턴 학습 가능 |


9. **모멘텀을 사용하는 이유는/모멘텀이 traditional 경사 하강법 알고리즘보다 나은 이유는?**      
모멘텀: 이전의 기울기 값의 일부를 현재 기울기 값에 추가해 가중치 갱신함(관성, momentum과 같은 효과)      
모멘텀의 장점: Local Minima, Saddle Point에 빠지지 않도록 함. 더 빠르게 모델 학습      
모멘텀을 사용한 최적화 알고리즘→ SGD(Stochastic Gradient Descent with Momentum), Adam, RMSProp, Adagrad

10. **엘라스틱넷을 사용하는 경우는 언제인가? 엘라스틱 넷을 사용하면 나타나는 단점은?**     
엘라스틱넷은 특징의 수가 샘플의 수보다 더 많을 때 쓰기 좋음.     
엘라스틱넷 → L1 L2 결합하여 사용하는 모델.     
더 많은 리소스를 씀, 모델 해석이 어려워지는 게(L1, L2 정칙화 둘다 사용하기 때문에) 단점     
     
11. **드롭아웃이란?**     
몇개의 노드를 학습에서 제외시켜버리는 것. 과적합 방지, 특정 뉴런에 의존하지 않도록 함.     

12. **데이터 증강이란?**     
데이터가 가진 고유한 특징을 유지한 채 변형하거나 노이즈를 추가해 **크기를 인위적으로 늘리는 방법**     
→과대적합 줄임(쓸데없는 데이터를 줄이고 꼭 필요한 데이터/알짜배기만 학습할 수 있도록 함).      
e.g. 텍스트 데이터 증강: 삽입, 삭제, 교체, 대체, 생성, 반의어, 맞춤법 교정, 역번역     
e.g. 이미지 데이터 증강: 회전, 대칭, 이동, 크기 조정     
***증강으로 데이터의 크기를 늘리면 더 다양한 데이터를 학습시킬 수 있어서 가지고 있는 정답(노이즈도 포함될 수 도 있음)만 학습하는게 아니라 일반적인 이미지들에 대한 성능을 키운다고 볼 수 있습니다**     

13. **백본이란?**      
뉴럴 네트워크에서 주요한 구조나 특성 추출하는 부분.     
**백본은 입력 데이터를 처리하고 이미지에서 특성을 추출하고 분류, 객체 탐지 등의 작업을 하기 전 기반을 닦아주는 역할을 합니다. 백본은 미리 트레이닝된 CNN(큰 데이터 셋으로 훈련됨)이 주입니다.**    
백본은 특성 추출 역할을 합니다.      
*이미지 -> 피쳐(행렬)로 바꿔주는 느낌입니다. 컴퓨터가 계산하고 이해할 수 있는 형태(숫자, 행렬)로 바꾸는 느낌입니다. 임베딩하는 느낌.     

"backbone" refers to the core architecture or feature extraction part of a neural network model. The backbone is responsible for processing input data, extracting meaningful features, and providing a foundation for subsequent tasks such as classification, object detection, or segmentation.     
A backbone network is typically a pre-trained convolutional neural network (CNN) that has been trained on a large dataset for a specific task, such as image classification.     
backbone in AI refers to the core architecture of a neural network that performs feature extraction.     

13. **전이학습이란?**     
이미 학습된 모델 참고함. 한 작업에서 학습된 모델을 내 데이터를 넣어서 재학습시킴         
→상대적으로 적은 양의 데이터로도 좋은 모델 만들기 가능.     
*뉴스 요약 모델 -> 영화 리뷰 요약 모델: 뉴스 요약 모델을 가져와서 영화 리뷰 데이터를 추가로 주면서 파인튜닝할 수 있습니다.
